INFO:     Started server process [389660]
INFO:     Waiting for application startup.
INFO:vlm-backend-vllm:Starting up vLLM Backend...
INFO:inference_engine:Initializing vLLM Async Inference Engine (TP=1, Ray)
INFO:inference_engine:Model path: /home/ec2-user/efs/vlm/experiments/phase3_qwen3_deepspeed/merged_model
/home/ec2-user/miniforge3/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field "model_identity" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 01-15 06:42:31 [model.py:514] Resolved architecture: Qwen3VLForConditionalGeneration
INFO 01-15 06:42:31 [model.py:1661] Using max model len 16384
INFO 01-15 06:42:31 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 01-15 06:42:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
INFO 01-15 06:42:31 [vllm.py:722] Cudagraph is disabled under eager mode
WARNING 01-15 06:42:31 [system_utils.py:136] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
/home/ec2-user/miniforge3/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field "model_identity" has conflict with protected namespace "model_".

You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
  warnings.warn(
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:42:38 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/ec2-user/efs/vlm/experiments/phase3_qwen3_deepspeed/merged_model', speculative_config=None, tokenizer='/home/ec2-user/efs/vlm/experiments/phase3_qwen3_deepspeed/merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/ec2-user/efs/vlm/experiments/phase3_qwen3_deepspeed/merged_model, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [2048], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=390317)[0;0m 2026-01-15 06:42:42,102	INFO worker.py:2007 -- Started a local Ray instance.
[0;36m(EngineCore_DP0 pid=390317)[0;0m /home/ec2-user/miniforge3/lib/python3.12/site-packages/ray/_private/worker.py:2046: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
[0;36m(EngineCore_DP0 pid=390317)[0;0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:42:43 [ray_utils.py:396] No current placement group found. Creating a new placement group.
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:42:48 [ray_env.py:66] RAY_NON_CARRY_OVER_ENV_VARS from config: set()
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:42:48 [ray_env.py:69] Copying the following environment variables to workers: ['LD_LIBRARY_PATH', 'VLLM_WORKER_MULTIPROC_METHOD']
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:42:48 [ray_env.py:74] If certain env vars should NOT be copied, add them to /home/ec2-user/.config/vllm/ray_non_carry_over_env_vars.json file
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m /home/ec2-user/miniforge3/lib/python3.12/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field "model_identity" has conflict with protected namespace "model_".
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m 
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m   warnings.warn(
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:   0% Completed | 0/14 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:   7% Completed | 1/14 [00:08<01:52,  8.66s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(pid=gcs_server)[0m [2026-01-15 06:43:10,076 E 390561 390561] (gcs_server) gcs_server.cc:303: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[0;36m(EngineCore_DP0 pid=390317)[0;0m [33m(raylet)[0m [2026-01-15 06:43:11,568 E 390812 390812] (raylet) main.cc:1032: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(bundle_reservation_check_func pid=391018)[0m [2026-01-15 06:43:12,588 E 391018 391730] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[2026-01-15 06:43:13,295 E 390317 391014] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  14% Completed | 2/14 [00:16<01:38,  8.20s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  21% Completed | 3/14 [00:24<01:28,  8.01s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(pid=391685)[0m [2026-01-15 06:43:13,283 E 391685 396204] core_worker_process.cc:842: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 95x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  29% Completed | 4/14 [00:32<01:19,  7.92s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  36% Completed | 5/14 [00:37<01:03,  7.01s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  43% Completed | 6/14 [00:45<00:58,  7.26s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  50% Completed | 7/14 [00:52<00:51,  7.40s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  57% Completed | 8/14 [01:00<00:44,  7.38s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  64% Completed | 9/14 [01:07<00:37,  7.45s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  71% Completed | 10/14 [01:15<00:29,  7.50s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  79% Completed | 11/14 [01:23<00:22,  7.54s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  86% Completed | 12/14 [01:30<00:15,  7.54s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards:  93% Completed | 13/14 [01:38<00:07,  7.57s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards: 100% Completed | 14/14 [01:45<00:00,  7.57s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m Loading safetensors checkpoint shards: 100% Completed | 14/14 [01:45<00:00,  7.56s/it]
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m 
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m WARNING 01-15 06:42:51 [worker_base.py:301] Missing `shared_worker_lock` argument from executor. This argument is needed for mm_processor_cache_type='shm'.
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:42:51 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:39025 backend=nccl
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:42:51 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:42:56 [gpu_model_runner.py:3562] Starting to load model /home/ec2-user/efs/vlm/experiments/phase3_qwen3_deepspeed/merged_model...
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:42:57 [mm_encoder_attention.py:104] Using AttentionBackendEnum.FLASH_ATTN for MMEncoderAttention.
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:42:57 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:44:43 [default_loader.py:308] Loading weights took 106.00 seconds
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:44:44 [gpu_model_runner.py:3659] Model loading took 62.4574 GiB memory and 106.424528 seconds
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:44:44 [gpu_model_runner.py:4446] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 1 image items of the maximum feature size.
[0;36m(EngineCore_DP0 pid=390317)[0;0m [36m(RayWorkerWrapper pid=391024)[0m INFO 01-15 06:54:53 [gpu_worker.py:375] Available KV cache memory: 6.54 GiB
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:54:54 [kv_cache_utils.py:1291] GPU KV cache size: 26,800 tokens
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:54:54 [kv_cache_utils.py:1296] Maximum concurrency for 16,384 tokens per request: 1.64x
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:54:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 610.42 seconds
[0;36m(EngineCore_DP0 pid=390317)[0;0m WARNING 01-15 06:55:00 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=390317)[0;0m INFO 01-15 06:55:00 [vllm.py:722] Cudagraph is disabled under eager mode
INFO:inference_engine:vLLM Async Engine initialized
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
[2026-01-15 07:57:52,832 E 390317 396249] rpc_client.h:203: Failed to connect to GCS within 60 seconds. GCS may have been killed. It's either GCS is terminated by `ray stop` or is killed unexpectedly. If it is killed unexpectedly, see the log file gcs_server.out. https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure. The program will terminate.
