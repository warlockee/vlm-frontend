[Unit]
Description=VLM Inference Service (vLLM Engine)
After=network.target

[Service]
Type=simple
User=ec2-user
Group=ec2-user
WorkingDirectory=/home/ec2-user/fsx/vlm-frontend/backend

# Resource Limits (Critical for Ray/NCCL)
LimitNOFILE=65535
LimitMEMLOCK=infinity

# Environment Setup
Environment="PATH=/home/ec2-user/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"
Environment="CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7"
Environment="LD_LIBRARY_PATH=/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/opt/amazon/openmpi/lib64:/opt/amazon/efa/lib64:/opt/amazon/ofi-nccl/lib64:/usr/local/lib:/usr/lib:/lib"
Environment="RAY_DEDUP_LOGS=0"

# Cleanup before start (removes stale sockets/zombies)
ExecStartPre=/home/ec2-user/fsx/vlm-frontend/backend/cleanup_service.sh

# Main Execution: Runs vlm_server.py on Port 8002
ExecStart=/home/ec2-user/miniconda3/bin/python3 -m uvicorn vlm_server:app --host 0.0.0.0 --port 8002

# Restart Policy
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
