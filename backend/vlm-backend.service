[Unit]
Description=VLM Gateway Service (Frontend Proxy)
After=network.target vlm-inference.service

[Service]
Type=simple
User=ec2-user
Group=ec2-user
WorkingDirectory=/home/ec2-user/fsx/vlm-frontend/backend

# Environment Setup
Environment="PATH=/home/ec2-user/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin"
# Gateway doesn't need GPUs
# Environment="CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7" 

# Environment Variables for Gateway
Environment="INFERENCE_URL=http://localhost:8002/inference"
Environment="TEACHER_URL=http://localhost:8002/inference"

# Main Execution: Runs main.py (Gateway) on Port 8000
ExecStart=/home/ec2-user/miniconda3/bin/python3 -m uvicorn main:app --host 0.0.0.0 --port 8000

# Restart Policy
Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
